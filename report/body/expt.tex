\section{实验与分析}

本章介绍实验设置及结果分析。其中，由于 Monte Carlo Learning 和 Q-Learning 受限于状态空间爆炸问题，仅在小规模地图上进行测试；
主要对 Approximate Q-Learning 算法进行了详细的对比实验和消融实验，以验证其有效性，并分析特征设计对性能的影响。

\subsection{实验设置}

\subsubsection{实验环境}

实验使用多种规模的地图进行测试。
填表式方法（MC learning 和 Q-Learning）由于状态空间爆炸问题，仅在小规模地图 \texttt{smallGrid}（$7 \times 7$，约 3 个食物）上进行训练和测试。
该地图包含 1 个鬼，没有胶囊。

Approximate Q-Learning 则在更大规模的地图上进行实验，包括：
\texttt{smallClassic}（$20 \times 7$，约 28 个食物），
\texttt{mediumClassic}（$20 \times 11$，约 48 个食物），
\texttt{originalClassic}（$28 \times 27$，约 240 个食物）。
这些地图均包含 2 个随机移动的鬼和若干胶囊（\texttt{small} 和 \texttt{medium} 中有 2 个，\texttt{original} 中有 4 个）。
鬼的移动策略为随机选择合法动作，攻击概率和逃跑概率均设为 0.2，引入一定的随机性。
图 \ref{fig:classic_layouts} 展示了三种 Classic 地图的结构。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.3\textwidth]{images/small_classic}
    \hfill
    \includegraphics[width=0.3\textwidth]{images/medium_classic}
    \hfill
    \includegraphics[width=0.3\textwidth]{images/original_classic}
    
    \vspace{0.3cm}
    
    \caption{实验使用的三种 Classic 地图：\texttt{smallClassic}（左）、\texttt{mediumClassic}（中）、\texttt{originalClassic}（右）}
    \label{fig:classic_layouts}
\end{figure}

\subsubsection{参数配置}

三种算法采用相同的超参数配置。
学习率 $\alpha = 0.1$，探索率 $\epsilon = 0.05$，折扣因子 $\gamma = 0.8$。
MC learning 和 Q-Learning 在 \texttt{smallGrid} 上训练 500 episode。
Approximate Q-Learning 在三种 Classic 地图上分别训练 1000 episode。
每个 episode 的最大步数限制为 1000 步，超过则强制终止。

\subsubsection{评价指标}

实验采用以下指标评估算法性能。
平均分数（Average Score）衡量智能体获得的平均总分，反映整体表现；
胜率（Win Rate）表示成功吃掉所有食物的 episode 比例，反映任务完成能力；
最高分数（Highest Score）记录测试中获得的最高分数，反映算法的性能上限。
所有指标均在训练完成后的 100 个测试 episode 上统计，测试时设置 $\epsilon = 0$ 、$\alpha = 0$。


\subsection{对比实验结果与分析}

\subsubsection{表格式方法在小规模地图上的表现}

表 \ref{tab:tabular_methods} 展示了 MC learning 和 Q-Learning 在 \texttt{smallGrid} 地图上的测试结果。

% TODO: 补充 MC learning 的数据
\begin{table}[h]
    \renewcommand{\arraystretch}{1.5}
    \setlength{\tabcolsep}{10pt}
    \centering
    \caption{表格式方法在 \texttt{smallGrid} 地图上的性能（训练 500 episode）}
    \small
    \begin{tabular}{lccc}
        \toprule
        \textbf{算法} & \textbf{平均分数} & \textbf{胜率 (\%)} & \textbf{最高分数} \\
        \midrule
        MC Learning & -7.8 & 50.0 & 499.0 \\
        Q-Learning & 436.0 & 94.0 & 500.0 \\
        \bottomrule
    \end{tabular}
    \label{tab:tabular_methods}
\end{table}

Q-Learning 在 \texttt{smallGrid} 上取得了 94\% 的胜率和 436.0 的平均分数，表明在状态空间较小的情况下，表格式方法能够有效学习最优策略。
MC Learning 的胜率为 50\%，平均分数为 -7.8，性能显著低于 Q-Learning。
这可能是因为 Q-Learning 采用单步 TD 更新，能够快速传播价值信息，
而 MC learning 需要等待完整 episode 才能更新，在长 episode 场景下学习效率较低，且对初期探索的依赖性更强。

然而，表格式方法的局限性在于无法扩展到更大规模的地图。
当尝试在 \texttt{smallClassic} 等地图上训练时，由于状态空间过大（约 $10^{17}$），智能体在有限的训练时间内无法充分访问所有状态，导致学习失败。
这一问题促使引入函数近似方法。

\subsubsection{Approximate Q-Learning 在不同规模地图上的表现}

表 \ref{tab:approx_q_performance} 展示了 Approximate Q-Learning 在三种不同规模地图上的测试结果。

\begin{table}[h]
    \renewcommand{\arraystretch}{1.5}
    \setlength{\tabcolsep}{10pt}
    \centering
    \caption{Approximate Q-Learning 在不同地图上的性能（训练 1000 episode）}
    \small
    \begin{tabular}{lccc}
        \toprule
        \textbf{地图} & \textbf{平均分数} & \textbf{胜率 (\%)} & \textbf{最高分数} \\
        \midrule
        \texttt{smallClassic} & 1332.1 & 89.0 & 1778.0 \\
        \texttt{mediumClassic} & 1602.1 & 84.0 & 2148.0 \\
        \texttt{originalClassic} & 2470.5 & 62.0 & 3834.0 \\
        \bottomrule
    \end{tabular}
    \label{tab:approx_q_performance}
\end{table}

从实验结果可以观察到，在 \texttt{smallClassic} 和 \texttt{mediumClassic} 地图上，Approximate Q-Learning 分别取得了 89\% 和 84\% 的胜率，表现稳定。
通过特征提取和线性函数近似，智能体能够有效泛化到未见过的状态，克服了表格式方法在大状态空间下的局限性。

随着地图规模继续增大，算法在 \texttt{originalClassic} 地图上的胜率降低至 62\%，相比小地图有所下降但仍保持较好的性能。
该地图的状态空间和食物数量远超训练环境（约 240 个食物），且地图结构更加复杂、鬼的数量更多。
这一结果表明函数近似方法具有较强的泛化能力。


\subsection{消融实验结果与分析}

\subsubsection{函数近似方法的组件消融}

为验证 Approximate Q-Learning 中各组件的作用，进行消融实验。
所有实验均在 \texttt{smallClassic} 地图上训练 1000 episode。

\begin{table}[h]
    \renewcommand{\arraystretch}{1.5}
    \setlength{\tabcolsep}{10pt}
    \centering
    \caption{Approximate Q-Learning 消融实验结果}
    \small
    \begin{tabular}{lccc}
        \toprule
        \textbf{方法配置} & \textbf{平均分数} & \textbf{胜率 (\%)} & \textbf{最高分数} \\
        \midrule
        完整方法 & 1332.1 & 89.0 & 1778.0 \\
        移除线性逼近 & 130.2 & 24.0 & 1310.0 \\
        移除特征提取 & \multicolumn{3}{c}{无法训练} \\
        \bottomrule
    \end{tabular}
    \label{tab:ablation_approx_q}
\end{table}

实验结果表明，线性函数近似对性能至关重要。
移除线性逼近后，仍使用特征向量表示状态，但采用表格式存储每个特征组合的 Q 值，性能大幅下降：平均分数从 1332.1 降至 130.2，胜率从 89\% 降至 24\%。
这是因为表格式方法无法泛化到未见过的特征组合。
相比之下，线性函数近似通过 $Q(s,a) = \mathbf{w} \cdot \mathbf{f}(s,a)$ 实现特征空间的插值，即使特征组合未在训练中出现，也能根据相似特征的权重给出合理估计。

移除特征提取后，算法相当于在原始状态空间上应用线性逼近。
由于 \texttt{smallClassic} 的状态空间过大（约 $10^{17}$），智能体在有限训练时间内无法充分探索，导致学习失败。
这一结果也验证了特征提取在降维中的必要性。

\subsubsection{特征设计的影响}

在 Approximate Q-Learning 基础上，进一步实验不同特征配置对性能的影响，重点分析标志位特征的作用。
实验均使用 \texttt{smallClassic} 地图，训练 1000 episode。

\begin{table}[h]
    \renewcommand{\arraystretch}{1.5}
    \setlength{\tabcolsep}{8pt}
    \centering
    \caption{特征配置对比实验}
    \small
    \begin{tabular}{lccc}
        \toprule
        \textbf{特征配置} & \textbf{平均分数} & \textbf{胜率 (\%)} & \textbf{最高分数} \\
        \midrule
        完整特征集 & 1332.1 & 89.0 & 1778.0 \\
        移除所有标志位特征 & 759.9 & 82.0 & 990.0 \\
        仅移除食物标志位 & 645.8 & 70.0 & 1374.0 \\
        \bottomrule
    \end{tabular}
    \label{tab:ablation_features}
\end{table}

实验结果表明，标志位特征对性能影响显著。
移除所有标志位特征（\texttt{eats-food}, \texttt{eats-scared-ghost}, \texttt{eats-capsule}）后，平均分数从 1332.1 下降至 759.9，胜率从 89\% 下降至 82\%。
进一步分析发现，仅移除食物标志位的影响更大，胜率降至 70\%，平均分数降至 645.8。

通过可视化观察，移除标志位特征后，智能体即使路过胶囊也不会主动吃掉。
这可能是因为胶囊本身不直接给分，仅通过距离特征无法学习到吃胶囊的长期价值；
而标志位特征提供了即时反馈，帮助智能体识别关键动作（如吃食物、吃胶囊、吃恐慌的鬼）的价值，从而更快地学习有效策略。

\subsection{训练过程观察}

\subsubsection{首次获胜现象分析}

实验中观察到，Approximate Q-Learning 在训练初期（通常前 2-3 个 episode）即可首次取得胜利。
然而通过可视化分析发现，此时智能体的策略尚不成熟。

具体表现为：智能体不会主动寻找远处的食物，当视野范围内无食物时倾向于停留或原地徘徊；仅在遇到鬼追赶时被动移动，若在移动过程中碰到食物才会继续得分。
这种早期获胜依赖于环境的随机性，智能体尚未学到系统性的搜索策略。

随着训练的进行，智能体逐渐学会主动规划路径、高效收集食物，胜率和平均分数稳步提升，最终收敛到稳定的性能水平。
这一现象反映了强化学习中探索与利用的平衡过程：初期随机探索可能偶然获得高回报，但需要通过持续学习才能形成稳定的最优策略。

\subsubsection{特征权重的变化}

训练过程中，特征权重的变化反映了智能体学习策略的过程。
图 \ref{fig:weight_evolution} 展示了主要特征权重在 \texttt{smallClassic} 地图 1000 轮训练中的变化趋势，按权重变化幅度分为三组。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{images/weight_evolution}
    \caption{主要特征权重随训练轮数的变化}
    \label{fig:weight_evolution}
\end{figure}

从权重变化曲线可以观察到以下现象。

图 (a) 展示了大幅度变化的特征。
\texttt{\#-of-normal-ghosts-1-step-away} 权重绝对值持续增长至超过 2500，且在整个训练过程中未见饱和趋势，表明回避危险鬼是学习的核心策略。

图 (b) 展示了中等幅度变化的特征。
\texttt{eats-food} 权重在训练初期快速增长，约 500 轮后稳定在 730 左右，说明智能体优先学会获取即时奖励。
\texttt{\#-of-scared-ghosts-1-step-away} 和 \texttt{scared-timer} 权重持续增长至约 500 和 550，反映了智能体逐渐学会利用鬼受惊状态的战术优势。
\texttt{eats-scared-ghost} 权重在训练约 150 轮后才开始明显增长，最终达到约 210，说明智能体在掌握基本生存策略后才学会这一进阶技巧。

图 (c) 展示了小幅度变化的特征。
距离类特征（\texttt{closest-food}、\texttt{closest-scared-ghost}、\texttt{closest-capsule}）
和 \texttt{eats-capsule} 的权重整体呈增长趋势，但变化幅度远小于其他特征。
这些特征的权重变化模式与预期存在差异，可能与特征设计或训练动态有关，具体原因尚不明确。

总体而言，权重变化呈现分阶段学习的特点：训练初期优先学会即时收益和危险回避，中期开始利用鬼受惊状态，后期逐渐优化长期规划。


% 关于线性逼近，即使不用线性逼近，也可以用 feature 替代 state 做表格，但这样仍然无法应对未见过的新状态


% 特征设计部分的尝试
% 没有标志位得分就会低很多 路过capsule也不会吃（可能是因为capsule本身不得分）



% 其他观察（新开一节？）
% firstWin
% 快的时候训练两到三个episode时agent就已经能取得胜利了
% 但此时可视化观察agent的行为，会发现agent不会主动去找远处的食物，眼前没有食物就会原地不动（会吃眼前的事物是标志位的做用吗？）；
% 只有遇到ghost才会被赶着跑，如果运气好遇到食物才会继续开始得分