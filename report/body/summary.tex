\section{总结}

从课堂、教材和网络资料中可以理解强化学习算法的数学原理和理论流程，但真正去实现代码、调试训练过程时仍会遇到许多问题。
而在修改代码的过程中，对于状态表示、动作选择、奖励设计等核心概念也会有新的认识。

% 实验中在环境搭建和特征设计部分花费的时间最长，主要是最初并不理解强化学习环境的接口规范，也不清楚状态空间和动作空间应该如何定义。
% 通过查找 Gymnasium 文档了解标准接口的要求；逐步实现状态从原始游戏数据到特征向量的转换过程，理解智能体如何感知环境；
% 对照各算法的更新公式，逐渐摸清 Q 值表、特征权重等结构的作用，之后才顺利完成算法实现。

在实现表格式方法（MC Learning、Q-Learning）时发现，即使是小规模的 7×7 地图，状态空间已经非常庞大，训练时很难遍历所有状态。
这让我真正体会到“维度灾难”不仅是理论上的概念，在实际应用中会严重制约算法的可行性。
使用 MC Learning 和 Q-Learning 测试时，即使加上特征提取，智能体仍然无法在稍大的地图上学会有效策略，
我花费了很多时间尝试改进特征设计，但效果很有限。
而在引入线性函数逼近方法后，智能体仅用 1000 个 episode 就学会了有效的策略，并且有很强的泛化能力。
在实现代码的过程中，我深入理解了 Q-Learning 和线性函数逼近的数学原理，
也深刻认识到算法设计的重要性。

% 在特征设计过程中收获颇丰，通过分析权重演化发现智能体会优先学习危险回避，然后才学会利用恐慌鬼获取高分，这种分阶段学习的现象很有意思。
但遗憾的是没有足够的时间去尝试更多的算法变体，在调试代码细节上花了比较多的时间，接触新方法较少。
此外，当前特征主要基于简单的距离计算，对于路径规划、区域控制等更高层次的策略缺乏建模，导致在复杂地图上的表现还不够理想。

未来还可以
尝试深度强化学习方法（如 DQN），用神经网络自动学习特征，避免手工设计的局限性；
以及引入更复杂的训练技巧，如经验回放、目标网络等，提高样本利用效率；

总体而言，本次实验让我对强化学习有了更深入的理解，不仅停留在理论层面，
而是通过实践熟悉了问题建模、算法设计、特征工程的过程，也认识到其中的种种挑战。